Often getting more training data to reduce the high variance is expensive. Also, adding more training data (X, Y ) increases the generalization accuracy until up-to only a certain limit. An alternative approach to improving the performance is using regularization. In regularization, we just add an extra term to our cost function which is the Frobenius norm of the weight matrix W. The parameter lambda is called as the regularization parameter which denotes the degree of regularization. Setting lambda to 0 results in no regularization, while large values of lambda correspond to more regularization. Lambda is usually set using cross validation. Regularization penalizes only the weights at each layer and leaves the biases un-regularized. This is mainly because the weight W has a lot of parameters ( each neuron of each hidden layer ) while b has just one parameter which means the biases typically require less data than the weights to Ô¨Åt accurately.


If we increase lambda, then weight W^[l] nearly tends to zero. Hence Z = W.X + B also becomes 0. This can make a neuron disappear in a layer and this zero propagates backward in the network making more neurons null/zero. Thus a complicated neural network becomes much simpler. However, if too many neurons get removed, we may just move from a scenario of high variance to high bias. Hence choosing an appropriate value of lambda is very important.

